from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import os
import logging
from datetime import datetime

from providers.llm_provider import LLMProviderFactory
from services.story_service import StoryService
from utils.rate_limiter import RateLimiter

from models.batch_models import BatchStoryRequest, BatchStoryResponse
from services.batch_story_service import BatchStoryService
from models.multiplayer_models import MultiplayerStoryRequest, MultiplayerStoryResponse
from services.multiplayer_story_service import MultiplayerStoryService

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="Behindy AI Server",
    description="ì§€í•˜ì²  ìŠ¤í† ë¦¬ ìƒì„± ì„œë¹„ìŠ¤ (ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸)",
    version="3.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
logger.info("ğŸš€ FastAPI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”")

logger.info("ğŸ“š BatchStoryService ì´ˆê¸°í™” ì¤‘...")
batch_story_service = BatchStoryService()
logger.info(f"âœ… BatchStoryService ì´ˆê¸°í™” ì™„ë£Œ: {batch_story_service}")

logger.info("ğŸ® MultiplayerStoryService ì´ˆê¸°í™” ì¤‘...")
multiplayer_story_service = MultiplayerStoryService()
logger.info(f"âœ… MultiplayerStoryService ì´ˆê¸°í™” ì™„ë£Œ: {multiplayer_story_service}")

logger.info("ğŸš¦ RateLimiter ì´ˆê¸°í™” ì¤‘...")
rate_limiter = RateLimiter()
logger.info(f"âœ… RateLimiter ì´ˆê¸°í™” ì™„ë£Œ: {rate_limiter}")

# FastAPI ë¯¸ë“¤ì›¨ì–´ë¡œ ëª¨ë“  ìš”ì²­ ë¡œê·¸
@app.middleware("http")
async def log_requests(request: Request, call_next):
    logger.info(f"ğŸŒ ë“¤ì–´ì˜¤ëŠ” ìš”ì²­: {request.method} {request.url}")
    logger.info(f"ğŸ”— í´ë¼ì´ì–¸íŠ¸ IP: {request.client.host}")
    logger.info(f"ğŸ“‹ í—¤ë”: {dict(request.headers)}")
    
    response = await call_next(request)
    
    logger.info(f"ğŸ“¤ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
    return response

# ===== í—¬ìŠ¤ì²´í¬ ë° ìƒíƒœ =====

@app.get("/")
async def root():
    """ê¸°ë³¸ í—¬ìŠ¤ ì²´í¬"""
    provider = LLMProviderFactory.get_provider()
    
    return {
        "message": "Behindy AI Server (Simplified)",
        "status": "healthy",
        "provider": provider.get_provider_name(),
        "timestamp": datetime.now().isoformat(),
        "version": "3.0.0",
        "endpoints": ["generate-complete-story", "multiplayer/next-phase", "health", "providers"]
    }

@app.get("/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ ì²´í¬"""
    provider = LLMProviderFactory.get_provider()
    available_providers = LLMProviderFactory.get_available_providers()
    
    return {
        "status": "healthy",
        "current_provider": provider.get_provider_name(),
        "available_providers": available_providers,
        "total_requests": rate_limiter.get_total_requests(),
        "timestamp": datetime.now().isoformat(),
        "simplified_mode": True
    }

@app.get("/providers")
async def get_providers_status():
    """Provider ìƒíƒœ í™•ì¸"""
    available_providers = LLMProviderFactory.get_available_providers()
    current_provider = LLMProviderFactory.get_provider()
    
    return {
        "current": current_provider.get_provider_name(),
        "available": available_providers,
        "settings": {
            "ai_provider": os.getenv("AI_PROVIDER", "mock"),
            "openai_model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            "claude_model": os.getenv("CLAUDE_MODEL", "claude-3-haiku"),
        }
    }

@app.post("/generate-complete-story", response_model=BatchStoryResponse)
async def generate_complete_story(request: BatchStoryRequest, http_request: Request):
    """
    ğŸ†• í†µí•© ìŠ¤í† ë¦¬ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
    - ì¼ë°˜ ìŠ¤í† ë¦¬ ìƒì„± (Spring Boot AIStoryService í˜¸ì¶œ)
    - ë°°ì¹˜ ìŠ¤í† ë¦¬ ìƒì„± (Spring Boot AIStoryScheduler í˜¸ì¶œ)
    - ëª¨ë“  ìŠ¤í† ë¦¬ ìƒì„± ìš”ì²­ì„ ì´ ì—”ë“œí¬ì¸íŠ¸ë¡œ ì²˜ë¦¬
    """
    try:
        # ğŸ†• ìš”ì²­ ì§„ì… ë¡œê·¸
        logger.info("=" * 80)
        logger.info("ğŸ¬ FastAPI /generate-complete-story ì—”ë“œí¬ì¸íŠ¸ ì§„ì…")
        logger.info(f"ğŸ“‹ ìš”ì²­ ë°ì´í„°:")
        logger.info(f"  station_name: {request.station_name}")
        logger.info(f"  line_number: {request.line_number}")
        logger.info(f"  character_health: {request.character_health}")
        logger.info(f"  character_sanity: {request.character_sanity}")
        logger.info(f"  story_type: {request.story_type}")
        logger.info(f"ğŸŒ HTTP ìš”ì²­ ì •ë³´:")
        logger.info(f"  í´ë¼ì´ì–¸íŠ¸ IP: {http_request.client.host}")
        logger.info(f"  User-Agent: {http_request.headers.get('user-agent', 'N/A')}")
        logger.info(f"  Content-Type: {http_request.headers.get('content-type', 'N/A')}")
        
        # ë‚´ë¶€ API í‚¤ ê²€ì¦ (ë°°ì¹˜ ìš”ì²­ì¸ ê²½ìš°)
        api_key = http_request.headers.get("X-Internal-API-Key")
        if api_key == "behindy-internal-2025-secret-key":
            logger.info("ğŸ”‘ ë‚´ë¶€ API í‚¤ ì¸ì¦ ì„±ê³µ (ë°°ì¹˜ ëª¨ë“œ)")
            request_mode = "BATCH"
        else:
            logger.info("ğŸ”“ ì¼ë°˜ API í˜¸ì¶œ (ê³µê°œ ëª¨ë“œ)")
            request_mode = "PUBLIC"
        
        logger.info(f"ğŸ“Š ìš”ì²­ ëª¨ë“œ: {request_mode}")
        logger.info("=" * 80)
        
        # Rate Limiting (ê³µê°œ ëª¨ë“œë§Œ ì ìš©)
        if request_mode == "PUBLIC":
            client_ip = http_request.client.host
            logger.info(f"ğŸš¦ Rate Limiting ì²´í¬ ì‹œì‘ (IP: {client_ip})")
            rate_limiter.check_rate_limit(client_ip)
            logger.info("âœ… Rate Limiting í†µê³¼")
        else:
            logger.info("ğŸš¦ Rate Limiting ê±´ë„ˆëœ€ (ë°°ì¹˜ ëª¨ë“œ)")
        
        logger.info(f"ğŸ¤– í˜„ì¬ Provider: {LLMProviderFactory.get_provider().get_provider_name()}")
        
        # ğŸ†• BatchStoryService í˜¸ì¶œ ì§ì „ ë¡œê·¸
        logger.info("ğŸ¬ BatchStoryService.generate_complete_story í˜¸ì¶œ ì‹œì‘")
        logger.info(f"  BatchStoryService ì¸ìŠ¤í„´ìŠ¤: {batch_story_service}")
        logger.info(f"  BatchStoryService íƒ€ì…: {type(batch_story_service)}")
        
        # ì™„ì „í•œ ìŠ¤í† ë¦¬ ìƒì„±
        response = await batch_story_service.generate_complete_story(request)
        
        # ğŸ†• BatchStoryService í˜¸ì¶œ ì™„ë£Œ ë¡œê·¸
        logger.info("âœ… BatchStoryService.generate_complete_story í˜¸ì¶œ ì™„ë£Œ")
        logger.info(f"ğŸ“¤ ì‘ë‹µ ë°ì´í„°:")
        logger.info(f"  story_title: {response.story_title}")
        logger.info(f"  theme: {response.theme}")
        logger.info(f"  pages_count: {len(response.pages)}")
        logger.info(f"  station_name: {response.station_name}")
        logger.info(f"  line_number: {response.line_number}")
        logger.info(f"  difficulty: {response.difficulty}")
        logger.info(f"  estimated_length: {response.estimated_length}")
        
        logger.info(f"ğŸ‰ ìŠ¤í† ë¦¬ ìƒì„± ì™„ë£Œ: {response.story_title}")
        logger.info("=" * 80)
        
        return response
        
    except HTTPException as e:
        logger.error(f"âŒ HTTPException ë°œìƒ:")
        logger.error(f"  ìƒíƒœì½”ë“œ: {e.status_code}")
        logger.error(f"  ìƒì„¸ ë‚´ìš©: {e.detail}")
        raise  # Rate limit ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
    except Exception as e:
        logger.error(f"âŒ ìŠ¤í† ë¦¬ ìƒì„± ì‹¤íŒ¨:")
        logger.error(f"  ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        logger.error(f"  ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
        logger.error(f"  ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
        
        # ğŸ†• ì—ëŸ¬ ë°œìƒì‹œ fallback ì‘ë‹µ
        logger.warning("âš ï¸ ì—ëŸ¬ ë°œìƒìœ¼ë¡œ fallback ì‘ë‹µ ìƒì„±")
        return BatchStoryResponse(
            story_title=f"{request.station_name}ì—­ì˜ ì´ì•¼ê¸°",
            description=f"{request.station_name}ì—­ì—ì„œ ë²Œì–´ì§€ëŠ” í¥ë¯¸ì§„ì§„í•œ ëª¨í—˜",
            theme="ë¯¸ìŠ¤í„°ë¦¬",
            keywords=[request.station_name, f"{request.line_number}í˜¸ì„ ", "ì§€í•˜ì² "],
            pages=[
                BatchPageData(
                    content=f"{request.station_name}ì—­ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì¼ì´ ë²Œì–´ì§‘ë‹ˆë‹¤. ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
                    options=[
                        BatchOptionData(
                            content="ì‹ ì¤‘í•˜ê²Œ í–‰ë™í•œë‹¤",
                            effect="sanity",
                            amount=2,
                            effect_preview="ì •ì‹ ë ¥ +2"
                        ),
                        BatchOptionData(
                            content="ë¹ ë¥´ê²Œ ëŒ€ì‘í•œë‹¤",
                            effect="health", 
                            amount=-1,
                            effect_preview="ì²´ë ¥ -1"
                        )
                    ]
                )
            ],
            estimated_length=1,
            difficulty="ë³´í†µ",
            station_name=request.station_name,
            line_number=request.line_number
        )

@app.post("/llm/multiplayer/next-phase", response_model=MultiplayerStoryResponse)
async def generate_multiplayer_story(request: MultiplayerStoryRequest, http_request: Request):
    try:
        logger.info("=" * 80)
        logger.info("ğŸ® ë©€í‹°í”Œë ˆì´ì–´ ìŠ¤í† ë¦¬ ìƒì„± ì—”ë“œí¬ì¸íŠ¸ ì§„ì…")
        logger.info(f"  station_name: {request.station_name}")
        logger.info(f"  line_number: {request.line_number}")
        logger.info(f"  current_phase: {request.current_phase}")
        logger.info(f"  participants: {len(request.participants)}")
        logger.info(f"  recent_messages: {len(request.recent_messages)}")

        api_key = http_request.headers.get("X-Internal-API-Key")
        if api_key != "behindy-internal-2025-secret-key":
            raise HTTPException(status_code=403, detail="Unauthorized internal API access")

        logger.info("ğŸ”‘ ë‚´ë¶€ API í‚¤ ì¸ì¦ ì„±ê³µ")
        logger.info(f"ğŸ¤– í˜„ì¬ Provider: {LLMProviderFactory.get_provider().get_provider_name()}")

        response = await multiplayer_story_service.generate_next_phase(request)

        logger.info("âœ… ë©€í‹°í”Œë ˆì´ì–´ ìŠ¤í† ë¦¬ ìƒì„± ì™„ë£Œ")
        logger.info(f"  content_length: {len(response.content)}")
        logger.info(f"  participant_updates: {len(response.participant_updates)}")
        logger.info("=" * 80)

        return response

    except HTTPException as e:
        logger.error(f"âŒ HTTPException: {e.status_code} - {e.detail}")
        raise
    except Exception as e:
        logger.error(f"âŒ ë©€í‹°í”Œë ˆì´ì–´ ìŠ¤í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        logger.error(f"  ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)

        from models.multiplayer_models import ParticipantUpdate
        return MultiplayerStoryResponse(
            content=f"{request.station_name}ì—­ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì¼ì´ ë²Œì–´ì§‘ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ ì„ íƒì´ í•„ìš”í•©ë‹ˆë‹¤.",
            summary=f"Phase {request.current_phase} ì§„í–‰ ì¤‘",
            participant_updates=[
                ParticipantUpdate(
                    character_name=p.character_name,
                    hp_change=-1,
                    sanity_change=-1
                )
                for p in request.participants
            ]
        )

# ===== ê´€ë¦¬ ë° ë””ë²„ê¹… API =====

@app.post("/validate-story-structure")
async def validate_story_structure(validation_request: Dict[str, Any], http_request: Request):
    """ìŠ¤í† ë¦¬ êµ¬ì¡° ê²€ì¦ (ë‚´ë¶€ API)"""
    try:
        # ë‚´ë¶€ API í‚¤ ê²€ì¦
        api_key = http_request.headers.get("X-Internal-API-Key")
        if api_key != "behindy-internal-2025-secret-key":
            raise HTTPException(status_code=403, detail="Unauthorized internal API access")
        
        logger.info("ğŸ” ìŠ¤í† ë¦¬ êµ¬ì¡° ê²€ì¦ ìš”ì²­")
        
        validation_result = await batch_story_service.validate_story_structure(
            validation_request.get("story_data", {})
        )
        
        return validation_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/batch/system-status")
async def get_batch_system_status(http_request: Request):
    """ë°°ì¹˜ ì‹œìŠ¤í…œ ìƒíƒœ (ë‚´ë¶€/ê³µê°œ API)"""
    try:
        # ë‚´ë¶€ API í‚¤ í™•ì¸ (ì„ íƒì )
        api_key = http_request.headers.get("X-Internal-API-Key")
        is_internal = api_key == "behindy-internal-2025-secret-key"
        
        provider = LLMProviderFactory.get_provider()
        available_providers = LLMProviderFactory.get_available_providers()
        
        status = {
            "ai_server_status": "healthy",
            "current_provider": provider.get_provider_name(),
            "available_providers": available_providers,
            "batch_service_ready": True,
            "rate_limit_status": {
                "total_requests": rate_limiter.get_total_requests(),
                "hit_rate": "N/A"
            },
            "timestamp": datetime.now().isoformat(),
            "simplified_mode": True,
            "version": "3.0.0"
        }
        
        # ë‚´ë¶€ ìš”ì²­ì¸ ê²½ìš° ë” ìƒì„¸í•œ ì •ë³´ ì œê³µ
        if is_internal:
            status["internal_mode"] = True
            status["api_endpoints"] = ["generate-complete-story"]
        
        return status
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ===== í…ŒìŠ¤íŠ¸ API =====

@app.post("/test-provider")
async def test_provider(test_request: Dict[str, Any]):
    """Provider í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸"""
    try:
        provider = LLMProviderFactory.get_provider()
        
        logger.info("ğŸ§ª Provider í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info(f"  í˜„ì¬ Provider: {provider.get_provider_name()}")
        logger.info(f"  í…ŒìŠ¤íŠ¸ ìš”ì²­: {test_request}")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í† ë¦¬ ìƒì„± ìš”ì²­
        test_request_obj = BatchStoryRequest(
            station_name=test_request.get("station_name", "ê°•ë‚¨"),
            line_number=test_request.get("line_number", 2),
            character_health=80,
            character_sanity=80,
            story_type="TEST"
        )
        
        test_result = await batch_story_service.generate_complete_story(test_request_obj)
        
        return {
            "provider": provider.get_provider_name(),
            "status": "success",
            "test_result": {
                "title": test_result.story_title,
                "theme": test_result.theme,
                "pages": len(test_result.pages)
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Provider í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return {
            "provider": "unknown",
            "status": "failed",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ===== í™˜ê²½ ì„¤ì • API =====

@app.get("/config")
async def get_config():
    """í˜„ì¬ í™˜ê²½ ì„¤ì • í™•ì¸"""
    return {
        "ai_provider": os.getenv("AI_PROVIDER", "mock"),
        "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
        "claude_configured": bool(os.getenv("CLAUDE_API_KEY")),
        "request_limits": {
            "per_hour": os.getenv("REQUEST_LIMIT_PER_HOUR", "100"),
            "per_day": os.getenv("REQUEST_LIMIT_PER_DAY", "1000")
        },
        "simplified_mode": True,
        "active_endpoints": [
            "/generate-complete-story",
            "/health", 
            "/providers",
            "/batch/system-status"
        ]
    }

# ===== ì—ëŸ¬ í•¸ë“¤ëŸ¬ =====

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    logger.warning(f"HTTP ì˜¤ë¥˜: {exc.status_code} - {exc.detail}")
    return {
        "error": exc.detail,
        "status_code": exc.status_code,
        "timestamp": datetime.now().isoformat()
    }

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"ì¼ë°˜ ì˜¤ë¥˜: {str(exc)}")
    return {
        "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "status_code": 500,
        "timestamp": datetime.now().isoformat()
    }

# ===== ì„œë²„ ì‹¤í–‰ =====

if __name__ == "__main__":
    import uvicorn
    
    # ì‹œì‘ì‹œ Provider ìƒíƒœ ë¡œê¹…
    try:
        provider = LLMProviderFactory.get_provider()
        available = LLMProviderFactory.get_available_providers()
        
        logger.info("=" * 60)
        logger.info("ğŸš€ Behindy AI Server ì‹œì‘ (Simplified)")
        logger.info(f"ğŸ“¡ í˜„ì¬ Provider: {provider.get_provider_name()}")
        logger.info(f"ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ Providers: {available}")
        logger.info("ğŸ¯ í™œì„±í™”ëœ ì—”ë“œí¬ì¸íŠ¸:")
        logger.info("  - POST /generate-complete-story (í†µí•© ìŠ¤í† ë¦¬ ìƒì„±)")
        logger.info("  - GET  /health (í—¬ìŠ¤ì²´í¬)")
        logger.info("  - GET  /providers (Provider ìƒíƒœ)")
        logger.info("  - GET  /batch/system-status (ì‹œìŠ¤í…œ ìƒíƒœ)")
        logger.info("ğŸ—‘ï¸  ì œê±°ëœ ì—”ë“œí¬ì¸íŠ¸:")
        logger.info("  - POST /generate-story (ì‚­ì œë¨)")
        logger.info("  - POST /continue-story (ì‚­ì œë¨)")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)